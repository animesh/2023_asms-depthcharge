{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting fragment ion intensity from peptide sequence\n",
    "\n",
    "In this vignette, we build Transformer model to predict the intensity of b- and y-series fragment ions for a peptide from its sequence and charge state, similar to Prosit, MS2PIP, and El Fragmentador. \n",
    "We'll use a subset the same ProteomeTools training, validation, and test data from [the original Prosit paper](https://www.nature.com/articles/s41592-019-0426-7).\n",
    "The data provided by Gessulat, et al is already preprocessed and in an HDF5 file format, which means that this notebook does not contain the code needed to generate annotations for training from arbitrary mass spectrometry data files.\n",
    "For that, we recommend looking at [this ProteomicsML tutorial](https://proteomicsml.org/tutorials/fragmentation/raw-to-prosit.html).\n",
    "\n",
    "**Before proceeding with this notebook, make sure to switch a GPU runtime on Google Colab.** To do this, click `Runtime` -> `Change runtime type`, and select `GPU` under `Hardware accelerator`. A new `GPU type` box should appear below. While any will work, we used the `T4` GPU to run this notebook previously.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The follow code installs the additional dependencies we'll need: Depthcharge, PyTorch Lightning, and Tensorboard. \n",
    "It also downloads the data that we'll be using from [FigShare](https://figshare.com/projects/prosit/35582). \n",
    "Please not that this data is several GB in size, so it may take a few minutes to download.\n",
    "In the end, we are left with our data in three HDF5 files in our working directory: `train.hdf5`, `valid.hdf5`, and `test.hdf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkwNS-qvTeUj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install lightning tensorboard depthcharge-ms\n",
    "for FILE in \"train.hdf5 24635459\" \"valid.hdf5 24635442\" \"test.hdf5 24635438\"\n",
    "do\n",
    "    set -- $FILE\n",
    "    if [ ! -f $1 ]; then\n",
    "        wget -nc https://figshare.com/ndownloader/files/$2\n",
    "        mv $2 $1\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries we'll need\n",
    "To work with our data, we'll need a handful of standard data science tools (NumPy, Pandas, etc.).\n",
    "For model building, we'll use PyTorch Lightning to wrap our model from Depthcharge, making it easy to train.\n",
    "\n",
    "From Depthcharge, we'll use the following classes:\n",
    "- `PeptideDataset` - This is a PyTorch Dataset that is designed to hold peptide sequences, their charge states, and additional metadata.\n",
    "- `FeedForward` - This is a utility PyTorch Module for quickly creating feed forward neural networks.\n",
    "- `PeptideTokenizer` - This class defines the amino acid alphabet, including modifications, that are valid tokens for our model. \n",
    "  It also tells Depthcharge how to convert a peptide sequence into tokens and back. \n",
    "  First-class support for ProForma formatted peptide sequences is included out-of-the-box.\n",
    "- `PeptideTransformerEncoder` - This is a PyTorch Module that embeds the peptide and its residues using a Transformer architecture.\n",
    "- `FloatEncoder` - This PyTorch Module encodes floating point numbers with sinusoidal waveforms. We use it here to encode the normalized collision energe (NCE)>\n",
    "\n",
    "After importing these libraries, the following code also sets a plotting theme and a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18129,
     "status": "ok",
     "timestamp": 1685310171518,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "3X0ta8JFpslm",
    "outputId": "db1480e5-468e-4a1d-f11e-7c936b2fc8d1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import einops\n",
    "import h5py\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from depthcharge.data import PeptideDataset\n",
    "from depthcharge.encoders import FloatEncoder\n",
    "from depthcharge.feedforward import FeedForward\n",
    "from depthcharge.tokenizers import PeptideTokenizer\n",
    "from depthcharge.transformers import PeptideTransformerEncoder\n",
    "\n",
    "# Set our plotting theme:\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Set random seeds\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tokenizer\n",
    "Now that we know the peptides that we want to consider, we need to create a tokenizer that accounts for all of the amino acids and modifications that may be present. Fortunately, the `PeptideTokenizer` class has a `from_proforma()` method that allows us to extract the amino acids and modifications from a collection of peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 4957,
     "status": "ok",
     "timestamp": 1685310176468,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "TFsgECNj7g54",
    "outputId": "61f0cac6-8a17-4299-d4c2-198cf8b4b630"
   },
   "outputs": [],
   "source": [
    "tokenizer = PeptideTokenizer.from_proforma(\n",
    "    sequences=\"ACDEFGHIKLMNPQRSTVWYM[Oxidation]\", \n",
    "    replace_isoleucine_with_leucine=False, \n",
    "    reverse=False,\n",
    ")\n",
    "\n",
    "pd.DataFrame(tokenizer.residues.items(), columns=[\"Token\", \"Mass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our tokenizer has captured all of the residues we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Datasets\n",
    "Now we need to prepare our PyTorch `Dataset`s and their corresponding PyTorch `DataLoader`s. \n",
    "Here, we use Depthcharge's `PeptideDataset` class, which handles transforming the peptide strings into PyTorch tensors for modeling. \n",
    "However, it takes some work to parse the data in the Prosit HDF5 file format, extract peptide sequence, and arrange the measured intensities in manner that is conducive for modeling with a Transformer.\n",
    "\n",
    "Our solution was to create a `PrositDataset` class, that is a subclass Depthcharge's `PeptideDataset` class.\n",
    "This class takes care of all these parsing and preprocessing steps.\n",
    "Additionally, this data is quite large and unable to fit in memory on our standard Google Colab instance. \n",
    "Although we could write a code to extract specific peptides from the HDF5 file dynamically, for the sake of brevity and speed we decided to extract a subset of each of the training, validation, and test datasets.\n",
    "We chose to subset each dataset by extracting every $k$th peptide, because the peptides are sorted lexographically within each file, and generating a truly random sample for this dataset is difficult.\n",
    "The `PrositDataset` chooses $k$ to yield approximately number of peptides requested.\n",
    "\n",
    "Running the code below may take a few minutes, but will yield the training, validation, and test data loaders that we need train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "joOG84U1piP9",
    "outputId": "e01452b0-0f29-4a1e-b520-1a4dddf0b638"
   },
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def vecs2seqs(vecs, alphabet):\n",
    "    \"\"\"Convert Prosit vectors to peptide sequenes\"\"\"\n",
    "    for idx, seq_idx in enumerate(vecs):\n",
    "        yield \"\".join([alphabet[i - 1] for i in seq_idx if i])\n",
    "\n",
    "@nb.njit\n",
    "def flip_and_shift_b_ions(ions, n_ions):\n",
    "    \"\"\"Flip the b_ions and shift them one down.\n",
    "\n",
    "    This let's them match the order of our Transformer model.\n",
    "    \"\"\"\n",
    "    for idx, n_ion in enumerate(n_ions): \n",
    "        ions[idx, 1:n_ion+1, 1, :] = ions[idx, n_ion-1::-1, 1, :]\n",
    "\n",
    "    return ions\n",
    "\n",
    "\n",
    "class PrositDataset(PeptideDataset):\n",
    "    \"\"\"A class for the Prosit HDF5 files.\"\"\"\n",
    "    def __init__(self, tokenizer, hdf5_file, max_examples=1_000_000):\n",
    "        \"\"\"Initialize the Prosit Dataset\"\"\"\n",
    "        alphabet = list(tokenizer.residues.keys())\n",
    "        with h5py.File(hdf5_file) as data:\n",
    "            n_rows = data[\"scan_number\"].shape[0]\n",
    "\n",
    "        if n_rows > max_examples:\n",
    "            # The peptides are lexigraphically sorted, so we'll take a \n",
    "            # diverse subset with creative indexing.\n",
    "            print(f\"  -> Found {n_rows} peptides. Subsetting to ~{max_examples}...\")\n",
    "            step = int(np.floor(n_rows / max_examples))\n",
    "        else:\n",
    "            step = 1\n",
    "\n",
    "        print(\"  -> Reading from HDF5 file....\")\n",
    "        with h5py.File(hdf5_file) as data:\n",
    "            charge = np.argmax(data[\"precursor_charge_onehot\"][::step], axis=1) + 1\n",
    "            charge = torch.tensor(charge).to(\"cuda\")\n",
    "            nce = torch.tensor(data[\"collision_energy_aligned_normed\"][::step]).to(\"cuda\")\n",
    "            seq = vecs2seqs(data[\"sequence_integer\"][::step], np.array(alphabet))\n",
    "            intensities = data[\"intensities_raw\"][::step]\n",
    "            n_rows = len(charge)\n",
    "\n",
    "        print(\"  -> Preprocessing intensities...\")\n",
    "        # Transform the intensities for our Transformer.\n",
    "        # Intensities are shape (L, I, Z) where:\n",
    "        # L = The peptide length - 1, ordered from lowest mass to highest.\n",
    "        # I = The ion series, (y, b)\n",
    "        # Z = The charge state (increasing)\n",
    "        intensities = intensities.reshape([n_rows, 29, 2, 3])\n",
    "        n_ions = (intensities[:, :, 0, 0] >= 0).sum(axis=1)\n",
    "        \n",
    "        # Need an extra space because we want to shift b ions.\n",
    "        intensities = np.pad(\n",
    "            intensities,\n",
    "            ((0, 0), (0, 1), (0, 0), (0, 0)), \n",
    "            \"constant\", \n",
    "            constant_values=-1,\n",
    "        )\n",
    "\n",
    "        intensities = flip_and_shift_b_ions(intensities, n_ions)\n",
    "        intensities[:, 0, 1, :] = -1\n",
    "        intensities = torch.tensor(intensities).to(\"cuda\")\n",
    "\n",
    "        print(\"  -> Tokenizing peptides...\")\n",
    "        super().__init__(tokenizer, seq, charge, nce, intensities)\n",
    "\n",
    "\n",
    "print(\"Loading the training dataset...\")\n",
    "train_dataset = PrositDataset(tokenizer, \"train.hdf5\", 200_000)\n",
    "print(\"Loading the validation dataset...\")\n",
    "validation_dataset = PrositDataset(tokenizer, \"valid.hdf5\", 100_000)\n",
    "print(\"Loading the test dataset...\")\n",
    "test_dataset = PrositDataset(tokenizer, \"test.hdf5\", 100_000)\n",
    "\n",
    "# The GPU memory on this instance is larger than the host, so\n",
    "# we put data on the gpu to run fast.\n",
    "for dset in (train_dataset, validation_dataset, test_dataset):\n",
    "    tensors = []\n",
    "    for data in dset.tensors:\n",
    "        tensors.append(data.to(\"cuda\"))\n",
    "\n",
    "    dset.tensors = tuple(tensors)\n",
    "\n",
    "train_loader = train_dataset.loader(\n",
    "    batch_size=128, shuffle=True,\n",
    ")\n",
    "validation_loader = validation_dataset.loader(\n",
    "    batch_size=1024, shuffle=False,\n",
    ")\n",
    "\n",
    "test_loader = test_dataset.loader(\n",
    "    batch_size=1024, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "\n",
    "Time to create a deep learning model using PyTorch Lightning and Depthcharge! \n",
    "Our model consists of a `PeptideTransformerEncoder` module to embed peptides and a `FeedForward` module to predict the intensity for charge states 1-3 of each expected b and y ion.\n",
    "With PyTorch Lightning, we need to specify the modules that comprise our model, define the optimizer(s) we will use to train it, and tell Lightning how to run the model when training, validating, and predicting.\n",
    "\n",
    "For this task, we're trying to minimize the spectral angle distance loss function:\n",
    "\n",
    "$$  L = \\frac{2}{n\\pi}\\sum^{n}_{i=1}\\cos^{-1}\\left( \\frac{Y_i \\cdot \\hat{Y}_i}{||Y_i|| \\cdot ||\\hat{Y}_i||}\\right)  $$\n",
    "\n",
    "Where $n$ is the number of peptides, $Y_i$ is a vector of measured intensities, and $\\hat{Y}_i$ is the vector of predicted intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FfpuXOUtL8xj"
   },
   "outputs": [],
   "source": [
    "def masked_spectral_angle(true, pred):\n",
    "    \"\"\"This is an PyTorch adaptation of the Prosit implementation here:\n",
    "    https://github.com/kusterlab/prosit/blob/dd16c47f8c3f4cfbd7ae84a1ca92a4d117e5e9ec/prosit/losses.py#L4-L16\n",
    "    \"\"\"\n",
    "    true = true.flatten(start_dim=1)\n",
    "    pred = pred.flatten(start_dim=1)\n",
    "    epsilon = torch.finfo(torch.float32).eps\n",
    "    pred_masked = ((true + 1) * pred) / (true + 1 + epsilon)\n",
    "    true_masked = ((true + 1) * true) / (true + 1 + epsilon)\n",
    "    pred_norm = F.normalize(true_masked, p=2, dim=-1)\n",
    "    true_norm = F.normalize(pred_masked, p=2, dim=-1)\n",
    "    product = torch.sum(pred_norm * true_norm, dim=1)\n",
    "    arccos = torch.acos(product)\n",
    "    return 2 * arccos / np.pi\n",
    "\n",
    "\n",
    "class FragmentPredictor(pl.LightningModule):\n",
    "    \"\"\"A Transformer model for CCS prediction\"\"\"\n",
    "    def __init__(self, tokenizer, d_model, n_layers):\n",
    "        \"\"\"Initialize the CCSPredictor\"\"\"\n",
    "        super().__init__()\n",
    "        self.peptide_encoder = PeptideTransformerEncoder(\n",
    "            n_tokens=tokenizer,\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "            max_charge=10,\n",
    "        )\n",
    "        self.nce_encoder = FloatEncoder(d_model, max_wavelength=1)\n",
    "        self.fragment_head = FeedForward(d_model, 6, 3)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        \"\"\"A training/validation/inference step.\"\"\"\n",
    "        seqs, charges, nce, intensities = batch\n",
    "        embedded, mask = self.peptide_encoder(seqs, charges)\n",
    "        emb_nce = self.nce_encoder(nce[:, None])\n",
    "        pred = self.fragment_head(embedded + emb_nce) \n",
    "\n",
    "        # Reshape for the Prosit data:\n",
    "        pred = einops.rearrange(pred, \"B L (I Z) -> B I Z L\", I=2)\n",
    "        pred = F.pad(pred, (0, 30 - pred.shape[-1]), \"constant\", 0)\n",
    "        pred = einops.rearrange(pred, \"B I Z L -> B L I Z\")\n",
    "\n",
    "        # Calculate the loss\n",
    "        if intensities is not None:\n",
    "            intensities = intensities.type_as(pred)\n",
    "            loss = masked_spectral_angle(intensities, pred).mean()\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"A training step\"\"\"\n",
    "        _, loss = self.step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"A validation step\"\"\"\n",
    "        _, loss = self.step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"An inference step\"\"\"\n",
    "        pred, _ = self.step(batch, batch_idx)\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure the optimizer for training.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Setup Tensorboard\n",
    "\n",
    "Tensorboard is a tool used to track the training progress of deep learning models in real time. \n",
    "Running the cell below will start Tensorboard and tell it to listen to the logs that will be written by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QED9S2nEWluw",
    "outputId": "01b0bdc8-c8a2-49de-d0b2-8b4092b6ddec"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "With our model defined and our data loaders ready to go, its time to fit the model to the data.\n",
    "The PyTorch Lightning `Trainer` will handle a lot of the training for us. \n",
    "We enable an early stopping criterium here, so that the trainer will stop once the loss on our validation dataset stops improving. \n",
    "This model should take <2 hours to train.\n",
    "If you've enabled Tensorboard in the previous cell, scroll back up while the model trains and you'll be able to watch its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYcxB_F9UKGY"
   },
   "outputs": [],
   "source": [
    "# Create a model:\n",
    "model = FragmentPredictor(tokenizer, d_model=64, n_layers=6)\n",
    "\n",
    "# Create the trainer:\n",
    "early_stopping = EarlyStopping(monitor=\"validation_loss\", patience=3)\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stopping],\n",
    "    max_epochs=10, \n",
    ")\n",
    "\n",
    "# Train the model:\n",
    "trainer.fit(\n",
    "    model=model, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=validation_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Validation dataset\n",
    "\n",
    "We now want to see how we've done, aside from just looking at the MSE. \n",
    "To get the predicte CCS for every peptide in our validation set, we use the `predict()` method for the trainer on our validation data loader.\n",
    "We then visualize our distribution of spectral angle distances using the empirical cumulative density function (ECDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.cat(trainer.predict(model, validation_loader)).cpu()\n",
    "sa_distance = masked_spectral_angle(validation_dataset.tensors[3].cpu(), preds).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "sns.ecdfplot(x=(1 - sa_distance), stat=\"count\")\n",
    "plt.xlabel(\"1 - Spectral Angle\")\n",
    "plt.ylabel(\"Number of Peptides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Test dataset\n",
    "\n",
    "Like with our validation data, we use the `predict()` method to get the predicted CCS for each of our test dataset peptides. \n",
    "We make a ECDF plot and save the predictions to a file, which we used to make the visualizations on our poster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.cat(trainer.predict(model, test_loader)).cpu()\n",
    "sa_distance = masked_spectral_angle(test_dataset.tensors[3].cpu(), preds).detach().numpy()\n",
    "\n",
    "plt.figure()\n",
    "sns.ecdfplot(x=(1 - sa_distance), stat=\"count\")\n",
    "plt.xlabel(\"1 - Spectral Angle\")\n",
    "plt.ylabel(\"Number of Peptides\")\n",
    "plt.show()\n",
    "\n",
    "torch.save(preds.detach().cpu(), \"test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "If you want to fully reproduce our figures from the poster, you'll need to clone [our GitHub repo](https://github.com/wfondrie/2023_asms-depthcharge), follow the instructions in the README for setting up your environment, and execute [intensity-prediction-figures.ipynb](https://github.com/wfondrie/2023_asms-depthcharge/blob/main/notebooks/intensity-prediction-figures.ipynb) Jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyONLhOSgQJd2HTaCfj3nSx1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
