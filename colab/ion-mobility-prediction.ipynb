{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Peptide Collisional Crossection from timsTOF data\n",
    "\n",
    "In this vignette, we build Transformer model to predict the measured collisional cross section (CCS) of a peptide from its sequence and charge state, using the same training and test data as [Meier et al](https://www.nature.com/articles/s41467-021-21352-8).\n",
    "To accomplish this task, we'll create a Transformer encoder for peptide sequences and charge states, and add a feed forward neural network to predict CCS from a single output token.\n",
    "\n",
    "**Before proceeding with this notebook, make sure to switch a GPU runtime on Google Colab.** To do this, click `Runtime` -> `Change runtime type`, and select `GPU` under `Hardware accelerator`. A new `GPU type` box should appear below. While any will work, we used the `T4` GPU to run this notebook previously.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The follow code installs the additional dependencies we'll need: Depthcharge, PyTorch Lightning, and Tensorboard. \n",
    "It also downloads the data that we'll be using, directly from the code repository from Meier et al, [here](https://github.com/theislab/DeepCollisionalCrossSection).\n",
    "In the end, we are left with our data in the working directory, `combined_sm.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49786,
     "status": "ok",
     "timestamp": 1685244106193,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "NCj-fMscvwej"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install lightning tensorboard depthcharge-ms\n",
    "wget -nc https://github.com/theislab/DeepCollisionalCrossSection/raw/master/data/combined_sm.csv.tar.gz\n",
    "tar -xzvf combined_sm.csv.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries we'll need\n",
    "To work with our data, we'll need a handful of standard data science tools (NumPy, Pandas, etc.).\n",
    "For model building, we'll use PyTorch Lightning to wrap our model from Depthcharge, making it easy to train.\n",
    "\n",
    "From Depthcharge, we'll use the following classes:\n",
    "- `PeptideDataset` - This is a PyTorch Dataset that is designed to hold peptide sequences, their charge states, and additional metadata.\n",
    "- `FeedForward` - This is a utility PyTorch Module for quickly creating feed forward neural networks.\n",
    "- `PeptideTokenizer` - This class defines the amino acid alphabet, including modifications, that are valid tokens for our model. \n",
    "  It also tells Depthcharge how to convert a peptide sequence into tokens and back. \n",
    "  First-class support for ProForma formatted peptide sequences is included out-of-the-box.\n",
    "- `PeptideTransformerEncoder` - This is a PyTorch Module that embeds the peptide and its residues using a Transformer architecture.\n",
    "\n",
    "After importing these libraries, the following code also sets a plotting theme and a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13957,
     "status": "ok",
     "timestamp": 1685244120139,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "-N6rbVrnwVNL",
    "outputId": "db2fa7eb-90ad-4f2d-c63b-bf6634ec4b57"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from depthcharge.data import PeptideDataset\n",
    "from depthcharge.feedforward import FeedForward\n",
    "from depthcharge.tokenizers import PeptideTokenizer\n",
    "from depthcharge.transformers import PeptideTransformerEncoder\n",
    "\n",
    "# Set our plotting theme:\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Set random seeds\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the data\n",
    "With our library loaded, we can now parse the CSV file from Meier et al. \n",
    "The peptide sequences are provided in a MaxQuant format, which we convert to be ProForma compliant.\n",
    "\n",
    "We then try and split the data in to training, validation, and test splits, matching the test data to that described in the paper;\n",
    "The paper states that the ProteomeTools subset was used as a test set, which are denoted by using the `PT` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4877,
     "status": "ok",
     "timestamp": 1685244125011,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "u8azBgm55Aln",
    "outputId": "98a3f0e4-f701-4bf0-affb-cd969871c5d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = (\n",
    "    pd.read_csv(\"combined_sm.csv\", index_col=0)\n",
    "    .sample(frac=1)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Modified sequence\": \"Seq\"})\n",
    ")\n",
    "\n",
    "# Convert sequences to ProForma compliant:\n",
    "data[\"Seq\"] = (\n",
    "    data[\"Seq\"]\n",
    "    .str.replace(\"_(ac)\", \"[Acetyl]-\", regex=False)\n",
    "    .str.replace(\"M(ox)\", \"M[Oxidation]\", regex=False)\n",
    "    .str.replace(\"_\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Verify we've accounted for all modificaâ€ ions:\n",
    "assert not data[\"Seq\"].str.contains(\"(\", regex=False).sum()\n",
    "\n",
    "# Split the data:\n",
    "test_df = data.loc[data[\"PT\"], :]\n",
    "data_df = data.loc[~data[\"PT\"], :]\n",
    "\n",
    "n_train = int(0.9 * len(data_df))\n",
    "train_df = data_df.iloc[:n_train, :].copy()\n",
    "validation_df = data_df.iloc[n_train:, :].copy()\n",
    "\n",
    "# Print the number in each set: \n",
    "print(\"Test peptides:                 \", len(test_df[\"Seq\"].unique()))\n",
    "print(\"Training + Validation peptides:\", len(data_df[\"Seq\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! The unique peptides precisely mach the numbers described by Meier et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tokenizer\n",
    "Now that we know the peptides that we want to consider, we need to create a tokenizer that accounts for all of the amino acids and modifications that may be present. Fortunately, the `PeptideTokenizer` class has a `from_proforma()` method that allows us to extract the amino acids and modifications from a collection of peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "executionInfo": {
     "elapsed": 27742,
     "status": "ok",
     "timestamp": 1685244152749,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "J1l3KylpVdsG",
    "outputId": "da595c55-8c3f-4e54-b576-d1a89498636c"
   },
   "outputs": [],
   "source": [
    "tokenizer = PeptideTokenizer.from_proforma(\n",
    "    sequences=validation_df[\"Seq\"], \n",
    "    replace_isoleucine_with_leucine=False, \n",
    "    reverse=False,\n",
    ")\n",
    "\n",
    "pd.DataFrame(tokenizer.residues.items(), columns=[\"Token\", \"Mass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our tokenizer has captured all of the residues we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Datasets\n",
    "Now we need to prepare our PyTorch `Dataset`s and their corresponding PyTorch `DataLoader`s. \n",
    "Here, we use Depthcharge's `PeptideDataset` class, which handles transforming the peptide strings into PyTorch tensors for modeling. \n",
    "Because this dataset is fairly small from a memory perspective, we go ahead and load it onto the GPU as well, to increase our training speed.\n",
    "Finally, the `loader()` method simplifies the creation of a PyTorch `DataLoader` for each dataset. \n",
    "\n",
    "We also transform the measured CCS using standard scaling, making it an easier value for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 101836,
     "status": "ok",
     "timestamp": 1685244904174,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "b69y-jG2J61i"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_dataset = PeptideDataset(\n",
    "    tokenizer,\n",
    "    train_df[\"Seq\"].to_numpy(), \n",
    "    torch.tensor(train_df[\"Charge\"].to_numpy()),\n",
    "    torch.tensor(scaler.fit_transform(train_df[[\"CCS\"]]).flatten()),\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataset = PeptideDataset(\n",
    "    tokenizer,\n",
    "    validation_df[\"Seq\"].to_numpy(),\n",
    "    torch.tensor(validation_df[\"Charge\"].to_numpy()),\n",
    "    torch.tensor(scaler.transform(validation_df[[\"CCS\"]]).flatten()),\n",
    ")\n",
    "\n",
    "test_dataset = PeptideDataset(\n",
    "    tokenizer,\n",
    "    test_df[\"Seq\"].to_numpy(),\n",
    "    torch.tensor(test_df[\"Charge\"].to_numpy()),\n",
    ")\n",
    "\n",
    "# This data is small so they can all live on the GPU:\n",
    "for dset in (train_dataset, validation_dataset, test_dataset):\n",
    "    tensors = []\n",
    "    for data in dset.tensors:\n",
    "        tensors.append(data.to(\"cuda\"))\n",
    "\n",
    "    dset.tensors = tuple(tensors)\n",
    "\n",
    "# Data loaders:\n",
    "train_loader = train_dataset.loader(batch_size=128, shuffle=True)\n",
    "validation_loader = validation_dataset.loader(batch_size=1024)\n",
    "test_loader = test_dataset.loader(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "\n",
    "Time to create a deep learning model using PyTorch Lightning and Depthcharge! \n",
    "Our model consists of a `PeptideTransformerEncoder` module to embed peptides and a `FeedForward` module to predict CCS from the latent representation. \n",
    "With PyTorch Lightning, we need to specify the modules that comprise our model, define the optimizer(s) we will use to train it, and tell Lightning how to run the model when training, validating, and predicting.\n",
    "\n",
    "For this task, we're trying to minimize the mean squared error (MSE) loss function:\n",
    "$$ L = \\frac{1}{n}\\sum^{n}_{i=1}(Y_i - \\hat{Y}_i)^2$$\n",
    "\n",
    "Where $n$ is the number of peptides, $Y$ is the measured CCS, and $\\hat{Y}_i$ is the predicted CCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1685244921935,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "9XepQ0TPErkY"
   },
   "outputs": [],
   "source": [
    "class CCSPredictor(pl.LightningModule):\n",
    "    \"\"\"A Transformer model for CCS prediction\"\"\"\n",
    "    def __init__(self, tokenizer, d_model, n_layers):\n",
    "        \"\"\"Initialize the CCSPredictor\"\"\"\n",
    "        super().__init__()\n",
    "        self.peptide_encoder = PeptideTransformerEncoder(\n",
    "            n_tokens=tokenizer,\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.ccs_head = FeedForward(d_model, 1, 3)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        \"\"\"A training/validation/inference step.\"\"\"\n",
    "        seqs, charges, ccs = batch\n",
    "        try:\n",
    "            embedded, _ = self.peptide_encoder(seqs, charges)\n",
    "        except IndexError as err:\n",
    "            print(batch)\n",
    "            raise err\n",
    "\n",
    "        pred = self.ccs_head(embedded[:, 0, :]).flatten()\n",
    "        if ccs is not None:\n",
    "            ccs = ccs.type_as(pred)\n",
    "            loss = torch.nn.functional.mse_loss(pred, ccs)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"A training step\"\"\"\n",
    "        _, loss = self.step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"A validation step\"\"\"\n",
    "        _, loss = self.step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"An inference step\"\"\"\n",
    "        pred, _ = self.step(batch, batch_idx)\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure the optimizer for training.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Setup Tensorboard\n",
    "\n",
    "Tensorboard is a tool used to track the training progress of deep learning models in real time. \n",
    "Running the cell below will start Tensorboard and tell it to listen to the logs that will be written by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QED9S2nEWluw"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "With our model defined and our data loaders ready to go, its time to fit the model to the data.\n",
    "The PyTorch Lightning `Trainer` will handle a lot of the training for us. \n",
    "We enable an early stopping criterium here, so that the trainer will stop once the MSE on our validation dataset stops improving. \n",
    "This model should take <2 hours to train.\n",
    "If you've enabled Tensorboard in the previous cell, scroll back up while the model trains and you'll be able to watch its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "TYcxB_F9UKGY",
    "outputId": "8de4a1f0-74fa-434b-d5ab-ac409be39658"
   },
   "outputs": [],
   "source": [
    "# Create a model:\n",
    "model = CCSPredictor(tokenizer, d_model=64, n_layers=4)\n",
    "comp_model = torch.compile(model)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"validation_loss\", patience=5)\n",
    "trainer = pl.Trainer(callbacks=[early_stopping], max_epochs=50)\n",
    "trainer.fit(\n",
    "    model=model, \n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=validation_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Validation dataset\n",
    "\n",
    "We now want to see how we've done, aside from just looking at the MSE. \n",
    "To get the predicted CCS for every peptide in our validation set, we use the `predict()` method for the trainer on our validation data loader.\n",
    "We then create a minimal scatter plot of the observed value against the predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_2PrF_QffZG"
   },
   "outputs": [],
   "source": [
    "pred = trainer.predict(model, validation_loader)\n",
    "validation_df = validation_df.copy()\n",
    "validation_df[\"pred\"] = scaler.inverse_transform(\n",
    "    torch.cat(pred).detach().cpu().numpy()[:, None]\n",
    ").flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(validation_df[\"CCS\"], validation_df[\"pred\"], s=1)\n",
    "plt.xlabel(\"Measured CCS\")\n",
    "plt.ylabel(\"Predicted CCS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good to me. \n",
    "If we want to perform further tweaks and optimizations, we should turn back and do them now. \n",
    "If not, we're ready to get our predictions for the test set, after which we should cease trying to optimize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Test dataset\n",
    "\n",
    "Like with our validation data, we use the `predict()` method to get the predicted CCS for each of our test dataset peptides. \n",
    "We make a similar scatterplot and save the data a a Parquet file, which we used to make the visualizations on our poster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1685244217055,
     "user": {
      "displayName": "Will Fondrie",
      "userId": "00004301715304315122"
     },
     "user_tz": 420
    },
    "id": "Vo6-0ropJrAT"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer()\n",
    "pred = trainer.predict(model, test_loader)\n",
    "\n",
    "test_df = test_df.copy()\n",
    "test_df[\"pred\"] = scaler.inverse_transform(\n",
    "    torch.cat(pred).detach().cpu().numpy()[:, None]\n",
    ").flatten()\n",
    "\n",
    "test_df.to_parquet(\"predictions.parquet\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(test_df[\"CCS\"], test_df[\"pred\"], s=1)\n",
    "plt.xlabel(\"Measured CCS\")\n",
    "plt.ylabel(\"Predicted CCS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! This looks great.\n",
    "\n",
    "If you want to fully reproduce our figures from the poster, you'll need to clone [our GitHub repo](https://github.com/wfondrie/2023_asms-depthcharge), follow the instructions in the README for setting up your environment, and execute [ccs-figures.ipynb](https://github.com/wfondrie/2023_asms-depthcharge/blob/main/notebooks/ccs-figures.ipynb) Jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOxLeq7ylWLebQkVHKS/qmC",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
